# Docker Compose for running NeuroSync (S1) and Eliza/the-org (S2)
# in a shared network for local development and testing the SCB bridge.
#
# Usage: docker compose -f docker-compose.yml -f docker-compose.bridge.yml up --build

services:
  neurosync:
    mem_limit: 12g
    build:
      context: ..
      dockerfile: ./NeuroBridge/dockerfile  # Builds combined Local API + Player image
    container_name: neurosync_s1
    command: [/app/entrypoint_bridge.sh]
    ports:
      - "5000:5000" # Expose Flask API port
      - "5001:5001" # Expose Player HTTP Server port
    environment:
      - FLASK_HOST=0.0.0.0
      - PYTHONUNBUFFERED=1
      - ALLOWED_ORIGINS=*
      - PLAYER_PORT=5001 # Set Player HTTP Server port
      # SCB storage
      - USE_REDIS_SCB=true
      - REDIS_URL=redis://redis:6379/0
      # - LIVELINK_UDP_IP=your_host_ip_address_here # Commented out to use default 'host.docker.internal'
      # TTS Configuration - Kokoro TTS Integration
      - TTS_PROVIDER=kokoro
      - KOKORO_TTS_SERVER_URL=http://host.docker.internal:6006
      - KOKORO_DEFAULT_VOICE=af_sarah
      - KOKORO_TTS_TIMEOUT=30
      - KOKORO_TTS_LANGUAGE=en
      # Add other NeuroSync env vars as needed (e.g., USE_REDIS_SCB)
    deploy: # Add deploy configuration for GPU access
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1 # or 'all'
              capabilities: [gpu]
    networks:
      - scb_bridge_net
    volumes:
      # Mount NeuroBridge code for live editing (optional)
      - ../NeuroBridge:/app/NeuroBridge
    env_file:
      - ../.env
    # Default CMD in image already starts Flask + client
    # Add healthcheck if desired
    # healthcheck:
    #   test: ["CMD", "curl", "--fail", "http://localhost:5000/scb/ping"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
  redis:
    image: redis:7-alpine
    container_name: redis_scb
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - scb_bridge_net
    restart: unless-stopped

  # PostgreSQL database for autonomous agent
  postgres:
    image: ankane/pgvector:latest
    container_name: autonomous_postgres_bridge
    environment:
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_USER=postgres
      - POSTGRES_DB=autonomous_agent
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - autonomous_postgres_data:/var/lib/postgresql/data:rw
    ports:
      - '127.0.0.1:5434:5432'  # Different port to avoid conflicts with other postgres instances
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -U $${POSTGRES_USER} -d $${POSTGRES_DB}']
      interval: 5s
      timeout: 5s
      retries: 5
    restart: always
    networks:
      - scb_bridge_net
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Database initialization service - runs analytics setup automatically
  db_init:
    image: postgres:16-alpine
    container_name: autonomous_db_init
    environment:
      - PGPASSWORD=postgres
    volumes:
      - ../tools/database/setup_analytics_tables.sql:/setup_analytics_tables.sql:ro
    networks:
      - scb_bridge_net
    depends_on:
      postgres:
        condition: service_healthy
    command: >
      sh -c "
      echo 'üîß [DB_INIT] Starting database initialization...';
      echo 'üìä [DB_INIT] Setting up analytics tables for autonomous agent...';
      psql -h postgres -U postgres -d autonomous_agent -f /setup_analytics_tables.sql;
      if [ $$? -eq 0 ]; then
        echo '‚úÖ [DB_INIT] Database analytics setup completed successfully!';
      else
        echo '‚ùå [DB_INIT] Database setup failed!';
        exit 1;
      fi;
      echo 'üéØ [DB_INIT] Autonomous agent database is ready for Phase 2!';
      "
    restart: "no"  # Only run once
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Cognee long-term memory service
  cognee:
    build:
      context: ../cognee-service
      dockerfile: Dockerfile
      args:
        LLM_API_KEY: ${OPENAI_API_KEY:-}
    container_name: cognee_server
    ports:
      - "8000:8000"
    env_file:
      - ../.env
    networks:
      - scb_bridge_net
    restart: unless-stopped

# --- RTMP server for low-latency audio/video sync ---
# Old block commented out due to indentation fix
#nginx-rtmp:
#  build: ./files_docker_rtmp/docker/rtmp
#  container_name: nginx_rtmp
#  ports:
#    - "1935:1935"   # RTMP ingest
#    - "8080:8080"   # HLS + status page
#  volumes:
#    - ./files_docker_rtmp/docker/rtmp/nginx.conf:/etc/nginx/nginx.conf
#  networks:
#    - scb_bridge_net
#  restart: unless-stopped
# --- RTMP server for low-latency audio/video sync ---
  nginx-rtmp:
    build:
      context: ../files_docker_rtmp/docker/rtmp
    container_name: nginx_rtmp
    ports:
      - "1935:1935"   # RTMP ingest
      - "8080:8080"   # HLS + status page & HLS manifest
    volumes:
      - ../files_docker_rtmp/docker/rtmp/nginx.conf:/etc/nginx/nginx.conf:ro
    networks:
      - scb_bridge_net
    restart: unless-stopped

  autonomous_starter:
    build:
      context: ../autonomous-starter          # directory with its Dockerfile
      dockerfile: Dockerfile                 # or omit if named "Dockerfile"
    container_name: autonomous_starter_s3
    working_dir: /app                        # adjust if your code lives elsewhere
    env_file:
      - ../.env
    ports:
      # inside the container it still binds to 3000,
      # but we publish it on 3100 so it doesn't fight with eliza_the_org
      - "3100:3000"
    environment:
      - PORT=3000                            # what the app listens on
      - LOG_LEVEL=debug
      # AI Provider Configuration - Use .env file values
      # Primary provider will be determined by available API keys
      # Livepeer configuration (fallback if no API keys)
      - LIVEPEER_GATEWAY_URL=https://dream-gateway.livepeer.cloud
      - LIVEPEER_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
      - LIVEPEER_LARGE_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct  
      - LIVEPEER_SMALL_MODEL=meta-llama/Meta-Llama-3.1-8B-Instruct
      - LIVEPEER_TEMPERATURE=0.6
      - LIVEPEER_MAX_TOKENS=2048
      # API keys loaded from .env file via env_file directive
      # VTuber and System Configuration
      - VTUBER_ENDPOINT_URL=http://neurosync:5001/process_text
      - NEUROSYNC_URL=http://neurosync:5000   # in case any other actions need it
      - NEUROSYNC_SCB_URL=http://neurosync:5000/scb/update
      - AUTONOMOUS_LOOP_INTERVAL=30000        # 30 seconds between iterations
      # Database Configuration - Connect to the postgres service in this network
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/autonomous_agent
      - POSTGRES_URL=postgresql://postgres:postgres@postgres:5432/autonomous_agent
      - DB_TYPE=postgres
      # Cognee Knowledge Graph Integration - NO Neo4j needed!
      - COGNEE_URL=http://cognee:8000
      - COGNEE_API_KEY=
    networks:
      - scb_bridge_net
    depends_on:
      postgres:
        condition: service_healthy
      neurosync:
        condition: service_started
      db_init:
        condition: service_completed_successfully
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Ollama Local LLM Server (GPU-enabled by default)
  ollama:
    image: ollama/ollama:latest
    container_name: vtuber-ollama
    ports:
      - "11434:11434"
    volumes:
      - ../ollama_data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_KEEP_ALIVE=24h
    restart: unless-stopped
    networks:
      - scb_bridge_net
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # Make this optional so the main system can run without it
    profiles:
      - ollama

networks:
  scb_bridge_net:
    driver: bridge

volumes:
  redis_data:
    driver: local
  autonomous_postgres_data:
    driver: local
    name: autonomous_postgres_bridge_data